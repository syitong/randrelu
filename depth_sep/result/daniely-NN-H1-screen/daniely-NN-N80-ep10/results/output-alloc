[[['log(rate)\\log(Gamma)', -4.0, -3.5, -3.0, -2.5, -2.0, -1.5, -1.0, -0.5], [-4.0, 0.6211803239557234], [-3.5, 0.5591030238261733], [-3.0, 0.5448199418310018], [-2.5, 0.5664069087702966], [-2.0, 0.6257815047128785], [-1.5, 0.6269526815108303], [-1.0, 0.7017417919190965], [-0.5, 0.5966823618708665]], {'dataset': 'daniely', 'N': 80, 'bd': 100000, 'mode': 'layer 2', 'n_epoch': 10, 'classes': [-1.0, 0.0, 1.0], 'loss_fn': 'squared', 'model': 'NN', 'feature': 'ReLU', 'logGamma': array([-4. , -3.5, -3. , -2.5, -2. , -1.5, -1. , -0.5]), 'lograte': array([-4. , -3.5, -3. , -2.5, -2. , -1.5, -1. , -0.5]), 'val_size': 2000, 'folds': 5, 'trials': 10, 'task': 'regression', 'gpu': 0, 'H': 1}]