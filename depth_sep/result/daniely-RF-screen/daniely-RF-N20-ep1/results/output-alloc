[[['log(rate)\\log(Gamma)', -4.0, -3.5, -3.0, -2.5, -2.0, -1.5, -1.0, -0.5], [-4.0, 423.96397217832777, 2.8316132721549745, 1.6618846697707699, 19.33788437212102, 2.432782940797226, 2.7365139372793146, 2.0544155058389615, 0.9358963442824495]], {'folds': 5, 'dataset': 'daniely', 'feature': 'ReLU', 'task': 'regression', 'model': 'RF', 'trials': 1, 'mode': 'layer 2', 'lograte': array([-4. , -3.5, -3. , -2.5, -2. , -1.5, -1. , -0.5]), 'bd': 100000, 'H': 2, 'gpu': 0, 'loss_fn': 'squared', 'val_size': 2000, 'N': 20, 'classes': [-1.0, 0.0, 1.0], 'logGamma': array([-4. , -3.5, -3. , -2.5, -2. , -1.5, -1. , -0.5]), 'n_epoch': 1}]